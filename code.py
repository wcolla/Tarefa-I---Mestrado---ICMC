import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import shapiro, ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from scipy.stats import zscore

#carregar os dados
df = pd.read_csv("/content/Steel_industry_data_task01.csv")
df.head()
df.info()
df.isnull().sum()
df.shape

# Apenas as linhas que tem NaN
nan_rows = df[df.isna().any(axis=1)]
print("Apenas linhas com NaN:")
print(nan_rows)

# NaN por coluna
print("Quantidade de NaN por coluna:")
print(df.isna().sum().sort_values(ascending=False))

# Dataframe booleano com a localiza√ß√£o dos NaNs
print("Localiza√ß√£o dos NaNs (True = √© NaN):")
print(df.isna())

# Colunas que tem algum NaN
print("Colunas com pelo menos um NaN:")
print(df.columns[df.isna().any()].tolist())

# 10 primeiras linhas com NaN
print("Amostra de linhas com NaN:")
print(df[df.isna().any(axis=1)].head(10))

df_copy = df.copy()


# Tratar outliers
# Converter a coluna de data
df['date'] = pd.to_datetime(df['date'], errors='coerce')

# Preencher NaNs num√©ricos com a mediana
numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))

# Preencher NaNs categ√≥ricos com a moda
categorical_cols = df.select_dtypes(include=['object']).columns
df[categorical_cols] = df[categorical_cols].apply(lambda x: x.fillna(x.mode()[0]))


# Utilizar t√©cnicas apresentadas em aula para substitui√ß√£o de dados e tratamento de outliers
# Fun√ß√£o para tratamento de outliers usando IQR
def treat_outliers_iqr(series):
  q1 = series.quantile(0.25)
  q3 = series.quantile(0.75)
  iqr = q3 - q1
  lower = q1 - 1.5 * iqr
  upper = q3 + 1.5 * iqr
  return np.where(series < lower, lower, np.where(series > upper, upper, series))

# Aplicar para colunas num√©ricas
for col in numeric_cols:
  df[col] = treat_outliers_iqr(df[col])


# Realizar avalia√ß√£o estat√≠stica dos dados
# Estat√≠ticas

# Moda
print("Moda (mais frequente):")
print(df.mode().iloc[0])

# Vari√¢ncia
print("\nVari√¢ncia:")
print(df.var(numeric_only=True))

# Assimetria
print("\nAssimetria (Skew):")
print(df.skew(numeric_only=True))

# Curtose
print("\nCurtose:")
print(df.kurt(numeric_only=True))

# Matriz de correla√ß√£o
print("Matriz de correla√ß√£o:")
print(df.corr(numeric_only=True))

# Verifica√ß√£o de valores negativos (em colunas que n√£o deveriam t√™-los)
print("Colunas com valores negativos:")
for col in df.select_dtypes(include=[np.number]):
    if (df[col] < 0).any():
        print(f"{col} tem valores negativos.")

# Comparar m√©dia vs. mediana (detec√ß√£o de assimetria ou outliers)
print("\n M√©dia vs Mediana por coluna num√©rica:")
for col in df.select_dtypes(include=[np.number]).columns:
    print(f"{col}: m√©dia = {df[col].mean():.2f}, mediana = {df[col].median():.2f}")

# Medidas de Dispers√£o
print("\nüìä Medidas de dispers√£o:")
print("Desvio padr√£o:\n", df.std(numeric_only=True))
print("\nCoeficiente de varia√ß√£o (%):")
print((df.std(numeric_only=True) / df.mean(numeric_only=True)) * 100)

# Teste de Normalidade
print("\nüìà Teste de normalidade (Shapiro-Wilk):")
for col in df.select_dtypes(include=[np.number]).columns:
    if df[col].count() >= 5000:  # shapiro tem limita√ß√£o
        sample = df[col].dropna().sample(5000, random_state=42)
    else:
        sample = df[col].dropna()
    stat, p = shapiro(sample)
    print(f"{col}: stat={stat:.4f}, p={p:.4f} {'(Normal)' if p > 0.05 else '(N√£o normal)'}")

# Teste de Hip√≥tese
print("\nüß™ Teste t de hip√≥tese: Usage_kWh entre 'Weekday' vs 'Weekend'")
df = df.dropna(subset=['Usage_kWh', 'WeekStatus'])

weekday = df[df['WeekStatus'] == 'Weekday']['Usage_kWh']
weekend = df[df['WeekStatus'] == 'Weekend']['Usage_kWh']

t_stat, p_val = ttest_ind(weekday, weekend, equal_var=False)
print(f"T-stat: {t_stat:.4f}, p-valor: {p_val:.4f} {'(Diferen√ßa significativa)' if p_val < 0.05 else '(Sem diferen√ßa significativa)'}")




# Codifica√ß√£o de vari√°veis categ√≥ricas
le_target = LabelEncoder()
df['Load_Type'] = le_target.fit_transform(df['Load_Type'])  # Alvo

# Codifique outras colunas com LabelEncoder novos
df['WeekStatus'] = LabelEncoder().fit_transform(df['WeekStatus'])
df['Day_of_week'] = LabelEncoder().fit_transform(df['Day_of_week'])

# Separando features e alvo
X = df.drop(columns=['Load_Type', 'date'])
y = df['Load_Type']

# Separa√ß√£o em treino/teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Normaliza√ß√£o
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dicion√°rio com os modelos
modelos = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Regress√£o Log√≠stica': LogisticRegression(max_iter=1000),
    '√Årvore de Decis√£o (CART)': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(kernel='rbf', C=1, gamma='scale')
}

# Codificando os r√≥tulos para os nomes de classe leg√≠veis, se necess√°rio
target_names = [str(label) for label in le_target.classes_]

# Treinamento e avalia√ß√£o com for
for nome, modelo in modelos.items():
    print(f"\nüîç Modelo: {nome}")

    # Alguns modelos usam dados normalizados, outros n√£o
    if nome in ['KNN', 'Regress√£o Log√≠stica', 'SVM']:
        modelo.fit(X_train_scaled, y_train)
        y_pred = modelo.predict(X_test_scaled)
    else:
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)



    # Avalia√ß√£o
    print(f"Acur√°cia: {accuracy_score(y_test, y_pred) * 100:.2f}%")
    print("Matriz de Confus√£o:\n", confusion_matrix(y_test, y_pred))
    print("Relat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_pred, target_names=target_names))



# Codifica√ß√£o de vari√°veis categ√≥ricas
le_target = LabelEncoder()
df_copy['Load_Type'] = le_target.fit_transform(df_copy['Load_Type'])
df_copy['WeekStatus'] = LabelEncoder().fit_transform(df_copy['WeekStatus'])
df_copy['Day_of_week'] = LabelEncoder().fit_transform(df_copy['Day_of_week'])

# Separar X e y antes da remo√ß√£o de outliers
X = df_copy.drop(columns=['Load_Type', 'date'])
y = df_copy['Load_Type']

# Normalizar para aplicar Z-score
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Calcular Z-score e remover outliers (Z > 3 ou < -3)
z_scores = zscore(X_scaled)
mask = ~(z_scores > 3).any(axis=1)  # Mant√©m apenas linhas sem outliers

X_no_outliers = X[mask]
y_no_outliers = y[mask]

# Garantir que n√£o restaram NaNs
X_no_outliers = X_no_outliers.dropna()
y_no_outliers = y_no_outliers.loc[X_no_outliers.index]

# Dividir treino/teste
X_train, X_test, y_train, y_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.3, stratify=y_no_outliers, random_state=42)

# Normaliza√ß√£o final
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dicion√°rio de modelos
modelos = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Regress√£o Log√≠stica': LogisticRegression(max_iter=1000),
    '√Årvore de Decis√£o (CART)': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(kernel='rbf', C=1, gamma='scale')
}

# Nomes das classes
target_names = [str(c) for c in le_target.classes_]

# Avalia√ß√£o dos modelos
for nome, modelo in modelos.items():
    print(f"\nüîç Modelo: {nome}")

    if nome in ['KNN', 'Regress√£o Log√≠stica', 'SVM']:
        modelo.fit(X_train_scaled, y_train)
        y_pred = modelo.predict(X_test_scaled)
    else:
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)

    print(f"Acur√°cia: {accuracy_score(y_test, y_pred) * 100:.2f}%")
    print("Matriz de Confus√£o:\n", confusion_matrix(y_test, y_pred))
    print("Relat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_pred, target_names=target_names))
